---
title: "Data Visualization Project 2: Analysis of the World Happiness dataset"
author: "Caroline Graebel, Nina Immenroth, Bogdan KostiÄ‡, Naim Zahari"
format:
  html:
    embed-resources: true
    number-sections: true
    toc: true
    toc_float: true
editor: source
---

# Top Level analysis of the data
To explore interesting structures in the data we will
use basic variable plotting around the main variable "happiness_score" to get a first feeling for the data. Afterwards we will look into how a RandomForest Classifier model would rate the importance of variables when trying to predict the happiness score. Lastly, a Kmeans model is used to look into a possible clustering of the data and how the resulting clusters can be interpreted.

```{r}
#| echo: false
#| output: false

# loading needed libraries
library(here)
library(tidyverse)
library(GGally)
library(grid)
library(randomForest)
library(corrplot)
```

```{r}
#| echo: false
# loading the needed data
whr <- read.csv(here::here("Data/WHR_AllYears.csv"))
```

## Introducing the variables {#sec-vars}

INSERT VARIABLE MEANINGS HERE

## Looking at distribution of the happiness score {#sec-distro}
```{r}
#| echo: false
#| label: fig-distro
#| fig-cap: "Distribution graph of the happiness score." 
#| fig-scap: "Distribution graph of the happiness score."
#| layout-ncol: 1
ggplot(whr, aes(x = happiness_score)) +
  geom_freqpoly(bins = 10) +
  ggtitle("Distribution of Happiness Score") +
  xlab("Happiness Score") +
  theme(text = element_text(size = 15))
```
When looking at our variable of interest, it's clearly visible that the distribution is quite close to a normal distribution.

## Looking into relationship of the happiness score to predictors {#sec-rel-pred}

### Region
```{r}
#| echo: false
#| label: fig-box-regions
#| fig-cap: "Boxplot Happiness Score based on different regions." 
#| fig-scap: "Boxplot Happiness Score based on different regions."
#| layout-ncol: 1
whr |>
  ggplot(aes(x = region, y = happiness_score, fill = region)) +
  geom_boxplot() +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        text = element_text(size = 14))+
  ggtitle("Happiness Score over different regions") +
  ylab("Happiness Score") +
  coord_flip()
```
When looking at how the happiness score is connected to the regions of the countries we have data for, we can see that they differ a lot in their spread. Western Europe and North America and ANZ have the comparatively highest median happiness scores. In contrast, for Sub-Saharan Africa and South Asia the median happiness is the lowest. The regions also differ strongly in the variance. The interquartile range (the range of variables within the first and third quarter percentiles) is larger in the Middle East and North Africa while for Africa and North America the boxes
are very small. <br>
Regions is understood as an aggregation for the variable country, as there are a big number of countries covered that it is not possible to visualize it. We are looking into countries that show interesting patterns later in the report.

```{r}
#| echo: false
#| output: false
#| label: fig-multiple-scatter
#| fig-cap: "Scatterplots for relationship of happiness score and different predictor variables." 
#| fig-scap: "Scatterplots for relationship of happiness score and different predictor variables."
#| fig-subcap:
#|   - "Happiness Score over GDP per Capita"
#|   - "Happiness Score over Social Support"
#|   - "Happiness Score over Healthy Life Expectancy"
#|   - "Happiness Score over Freedom to make Life Choices"
#|   - "Happiness Score over Generosity"
#|   - "Happiness Score over Perceptions of Corruption"
#| layout-ncol: 2

# GDP per capita plot
whr |>
  ggplot(aes(x = gdp_per_capita, y = happiness_score)) +
  geom_point() +
  xlab("GDP per Capita") +
  ylab("Happiness Score") +
  theme(text = element_text(size = 15))

# Social Support Plot
whr |>
  ggplot(aes(x = social_support, y = happiness_score)) +
  geom_point() +
  xlab("Social Support") +
  ylab("Happiness Score") +
  theme(text = element_text(size = 15))

# Healthy Life Expectancy plot
whr |>
  ggplot(aes(x = healthy_life_expectancy, y = happiness_score)) +
  geom_point() +
  xlab("Healthy Life Expectancy") +
  ylab("Happiness Score") +
  theme(text = element_text(size = 15))

# Freedom to make life choices
whr |>
  ggplot(aes(x = freedom_to_make_life_choices, y = happiness_score)) +
  geom_point() +
  xlab("Freedom to make Life Choices") +
  ylab("Happiness Score") +
  theme(text = element_text(size = 15))

# Generosity
whr |>
  ggplot(aes(x = generosity, y = happiness_score)) +
  geom_point() +
  xlab("Generosity") +
  ylab("Happiness Score") +
  theme(text = element_text(size = 15))

# Perceptions of corruption
# fix data type of perceptions of corruption
whr$perceptions_of_corruption <- as.numeric(whr$perceptions_of_corruption)
whr |>
  ggplot(aes(x = perceptions_of_corruption, y = happiness_score)) +
  geom_point() +
  xlab("Perceptions of corruption") +
  ylab("Happiness Score") +
  theme(text = element_text(size = 15))

```

```{r}
#| echo: false
#| label: fig-corr-mat
#| fig-cap: "Correlation matrix plot." 
#| fig-scap: "Correlation matrix plot."
#| layout-ncol: 1

#remove non-numeric variables
whr_num <- whr[, c(3,4,5,6,7,8,9,10)]
whr_num$perceptions_of_corruption <- as.numeric(whr_num$perceptions_of_corruption)
whr_num$year <- as.numeric(whr_num$year)

# remove NaN values
whr_num_nonan <- na.omit(whr_num)

# calculate correlation
whr_cor <- cor(whr_num_nonan)

# shorten the variable names for the plot
new_names <- c("HS", "GpC", "SS", "HLE", "FtmLC", "G", "PoC", "Y")
colnames(whr_cor) <- new_names
rownames(whr_cor) <- new_names

# calculate p-values
testRes <- cor.mtest(whr_num_nonan, conf.level = 0.95)
colnames(testRes$p) <- new_names
rownames(testRes$p) <- new_names

corrplot(whr_cor, p.mat = testRes$p, method = 'circle', type = 'lower', insig='blank',
         addCoef.col ='black', number.cex = 0.8, order = 'AOE', diag=FALSE)
```

### GDP per Capita
There seems to be a positive linear relationship between GDP per capita and the happiness score, as with higher GDP the happiness score rises. The relationship is stronger than it seems from the plot, as the scale ratio of x- and y-axis are so different. Calculating the correlation helps to provide further proof for the linear relationship between the variables.
```{r}
#| echo: false
# correlation of gdp per capita and happiness score
cat("Correlation of the happiness score and GDP per capita:",cor(whr$gdp_per_capita, whr$happiness_score))
```
The correlation of these variables, which can be seen in figure @fig-corr-mat, is very high with a value of 0.72.

### Social Support

Similar to GDP per Capita, there is a positive linear relationship between both variables.
```{r}
#| echo: false
cat("Correlation of the happiness score and social support:",cor(whr$social_support, whr$happiness_score))
```
The correlation between social support and happiness score isn't as strong as for GDP per capita but still very strong.

### Healthy life expectancy
Here, we again have a positive linear relationship between the two variables. Interestingly, for healthy life expectancy some values are missing.
```{r}
#| echo: false
cat("Correlation of the happiness score and healthy life expectancy:",cor(whr$healthy_life_expectancy, whr$happiness_score, use = "complete.obs"))
```
There a strong positive correlation between healthy life expectancy and the happiness score.

### Freedom to make life choices

In this case, there also is a positive linear relationship between freedom to make life choices and happiness score, even though it seems a bit weaker than with the variables discussed so far.
```{r}
#| echo: false
cat("Correlation of the happiness score and freedom to make life choices:",cor(whr$freedom_to_make_life_choices, whr$happiness_score))
```
In general, there is a strong positive correlation between the variables but it is the weakest so far.

### Generosity
There is no connection between the variables visible here.

### Perceptions of corruption
For perceptions of corruption, there seems to be a slight upward trend in happiness score for higher perceptions of corruption but it's only a fraction of data points that score high on perceptions of corruption. Also there is missing data for this variable.
```{r}
#| echo: false
cat("Correlation of the happiness score and Perceptions of corruption:",
    cor(whr$perceptions_of_corruption, whr$happiness_score, use = "complete.obs"))
```
Calculating the correlation shows that there is again a positive correlation between the two variables, even if it appears weak compared to other variables that have been looked at.

### Year
```{r}
#| echo: false
#| label: fig-box-year
#| fig-cap: "Boxplot Happiness Score over Years." 
#| fig-scap: "Boxplot Happiness Score over Years."
#| layout-ncol: 1
whr$year <- as.factor(whr$year)
whr |>
  ggplot(aes(x = year, y = happiness_score, fill = year)) +
  geom_boxplot() +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        text = element_text(size = 15))+
  ylab("Happiness Score")
```
For medians, there is a slight upwards trend over the years, which 2020 to 2022 having little variance compared to the other years. There are also negative outliers for 2021 to 2023.

# Using K-means Clustering to find interesting patterns
It is the goal to use K-means clustering to provide a pattern that is interpretable and give further context to the relationship between the happiness score and other variables. The procedure will first be introduced, then the data is scaled for an optimal performance and the best amount of clusters will be chosen by using the elbow-criteria to sensibly minimize total within sum-of-squares <span style="color:red">Explain WSS</span>. At the end, a final model is trained and the result is plotted. <br>
It is important to mention that K-Means can only be used for continuous variables, so country, region, and year are not considered here.

## Introduction K-Means
K-Means Clustering is an unsupervised machine learning method that can help classify data that has no label that a model could be trained on. This is made possible by a distance-based approach that fits the clusters so that the distances between the data points within a cluster are minimal. The distance in this case is a measure of similarity between data points, so in other words we want to fit clusters so that the points contained in one cluster are as similar as possible.

## Trying PCA to improve K-Means performance
```{r}
#| echo: false
# create a subset that doesn't contain country, region and year
whr_sub <- whr[,-c(1,2,10)]

# delete rows that contain NAs from the data frame
whr_sub <- na.omit(whr_sub)
```
A good measure for improving the performance of K-Means is doing PCA on the data as it can aggregate the information. However, PCA is only useful if you can cover around 80% of the data with only few principle components.
```{r}
#| echo: false
pr.out <- prcomp(whr_sub, scale = TRUE)

# get percent of variance of each principal component
perc <- pr.out$sdev/sum(pr.out$sdev)
```

```{r}
#| echo: false
#| label: fig-pca-cum
#| fig-cap: "Cumulative Propability of the Variance explained by each Principle Component." 
#| fig-scap: "Cumulative Propability of the Variance explained by each Principle Component."
#| layout-ncol: 1
# plot cumulative sum of the percentages
plot(cumsum(perc),
  xlab = "Principal Component",
  ylab = "Proportion of Variance Explained",
  ylim = c(0,1),
  type = "b")
```
As can be seen, it would be necessary to use four to five of seven variables to cover a sufficient amount of variance. Since this isn't too helpful and each principle component contains a similar amount of variance, there will be no PCA used before doing K-Means.

## Finding a good amount of clusters
In general, the higher the number of clusters, the more similar points will be within one cluster. However, the model also gets harder to interpret and messy. So using the elbow-criteria, the hyperparameter k that equals the number of clusters will be sensibly minimized so that we get an interpretable result. <br>
We test the total within sum of squares for two to ten clusters.
```{r}
#| echo: false
# scale data
sc_whr <- scale(whr_sub)

# initiate vector to store total within sum of squares for each kmeans()-object
wss <- c()

# run kmeans for k equals two to seven and save total within sum of squares
for (k in 2:7) {
  km.out <- kmeans(sc_whr, centers = k, nstart = 20)
  wss <- append(wss, km.out$tot.withinss)
}
```

```{r}
#| echo: false
#| label: fig-sos
#| fig-cap: "Within sum of squares over different amounts of clusters." 
#| fig-scap: "Within sum of squares over different amounts of clusters."
#| layout-ncol: 1
# plot the resulting values
plot(wss, xlab = "k", ylab = "WSS", type = "b")
```
Using the elbow criterium, it can be seen that after k = 2, the decrease of within sum of squares isn't as strong anymore. Therefore, the final K-Means model is trained with two clusters.

## Resulting plots
```{r}
#| echo: false
# train K-Means
km.out <- kmeans(sc_whr, centers = 2, nstart = 20)

my.text.panel <- function(labels) {
  function(x, y, lbl, ...) {
    if (lbl %in% names(labels)) lbl <- labels[[lbl]]
    text(x, y, lbl, ...)
  }
}
```

```{r fig.width=10, fig.height=10}
#| echo: false
#| label: fig-k-means-plot
#| fig-cap: "Matrix of scatterplots coloured by cluster." 
#| fig-scap: "Matrix of scatterplots coloured by cluster."
#| layout-ncol: 1
# plot resulting clustering
pairs(sc_whr, col = km.out$cluster, upper.panel = NULL, gap = 0,
      text.panel = my.text.panel(c(happiness_score="Happiness Score",
                                   gdp_per_capita="GDP p. Capita",
                                   social_support="Social Support",
                                   healthy_life_expectancy="HL Expectancy",
                                   freedom_to_make_life_choices = "Freedom Life Choices",
                                   generosity = "Generosity",
                                   perceptions_of_corruption = "Perceived corruption")))
```
When looking at the scatterplots, the first thing to be noted is that for happiness score, there is
the cleanest split between the clusters which basically splits the plot into a happiness score that is bigger than 0 and vice versa. In other words, the clusters are strongly informed by whether the happiness score is in the upper 50% quantile or below.

```{r}
#| echo: false
sc_whr <- as.data.frame(sc_whr)
cat("Median of the happiness score:",median(sc_whr$happiness_score))
```
For further context, the median value of the happiness score is almost perfectly zero. From the initial plots it has been shown that the happiness score correlates with all variables plotted here except for generosity. From the matrix of scatterplots we can gather that all plots within the matrix show the same pattern of the red cluster being in the lower left and the black cluster in the upper.<span style="color:red">Last sentence?</span>

