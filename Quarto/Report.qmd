---
title: "Data Visualization Project 2: Analysis of the World Happiness dataset"
author: "Caroline Graebel, Nina Immenroth, Bogdan KostiÄ‡, Naim Zahari"
format:
  html:
    embed-resources: true
    number-sections: true
    toc: true
    toc_float: true
editor: source
---

# Top Level analysis of the data
To explore interesting structures in the data worth looking into we will
use basic variable plotting around the main variable "happiness_score" to get a first feeling for the data. Afterwards we will look into how a RandomForest Classifier model would rate the importancy of variables when trying to predict the happiness score. Lastly, a Kmeans model is used to look into a possible clustering of the data and how the resulting clusters can be interpreted.

## Loading the packages
```{r}
#| output: false
 
# loading needed libraries
library(here)
library(tidyverse)
library(GGally)
```

## Loading the data
```{r}
#| output: false

# loading the needed data
whr <- read.csv(here::here("Data/WHR_AllYears.csv"))
```

## Introducing the variables

INSERT VARIABLE MEANINGS HERE

## Looking into relationship of "happiness_score" and potential predictors

### Region
```{r}
whr |>
  ggplot(aes(x = region, y = happiness_score, fill = region)) +
  geom_boxplot() +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        text = element_text(size = 14))+
  ggtitle("Happiness Score over different regions") +
  ylab("Happiness Score") +
  coord_flip()
```
When looking at how the Happiness score is connected to the regions of the countries we have data for, we can see that they differ a lot in their spread. Western Europe and North America and ANZ have the comparitively highest median happiness scores. In contrast, for Sub-Saharan Africa and South Asia the median happiness is the lowest. The regions also differ strongly in the variance. The variables within the first and third quarter percentiles cover a big range in the Middle East and North Africa while for Africa and North America the boxes
are very small. <br>
For the following looks at the variables, regions is understood as an aggregation for the variable country, as there are a big number of countries covered that it is not possible to visualize it. We are looking into countries that show interesting patterns later in the report.

### GDP per Capita
```{r}
whr |>
  ggplot(aes(x = gdp_per_capita, y = happiness_score)) +
  geom_point() +
  xlab("GDP per Capita") +
  ylab("Happiness Score") +
  ggtitle("Happiness Score over GDP per Capita") +
  theme(text = element_text(size = 15))
```
There seems to be a positive linear relationship between GDP per capita and the happiness score, as with higher GDP the happiness score rises. The relationship is stronger than it seems from the plot, as the scale ratio of x- and y-axis are so different. Calculating the correlation helps with providing further prove for the linear relationship between the variables.
```{r}
# correlation of gdp per capita and happiness score
cor(whr$gdp_per_capita, whr$happiness_score)
```
The correlation of these variables - as expected - is very high.

### Social Support
```{r}
whr |>
  ggplot(aes(x = social_support, y = happiness_score)) +
  geom_point() +
  xlab("Social Support") +
  ylab("Happiness Score") +
  ggtitle("Happiness Score over Social Support") +
  theme(text = element_text(size = 15))
```
Similar to GDP per Capita, there is a positive linear relationship between both variables.
```{r}
cor(whr$social_support, whr$happiness_score)
```
The correlation between social support and happiness score isn't as strong as for GDP per capita but still very strong.

### Healthy life expectancy
```{r}
whr |>
  ggplot(aes(x = healthy_life_expectancy, y = happiness_score)) +
  geom_point() +
  xlab("Healthy Life Expetancy") +
  ylab("Happiness Score") +
  ggtitle("Happiness Score over Healthy Life Expetancy") +
  theme(text = element_text(size = 15))
```
Here, we again have a positive linear relationship between the two variables. Interestingly, for healthy life expectancy some values are missing.
```{r}
cor(whr$healthy_life_expectancy, whr$happiness_score, use = "complete.obs")
```
There a strong positive correlation between healthy life expectancy and the happiness score.

### Freedom to make life choices
```{r}
whr |>
  ggplot(aes(x = freedom_to_make_life_choices, y = happiness_score)) +
  geom_point() +
  xlab("Freedom to make Life Choices") +
  ylab("Happiness Score") +
  ggtitle("Happiness Score over Freedom to make Life Choices") +
  theme(text = element_text(size = 15))
```
In this case, there also is a positive linear relationship between freedom to make life choices and happiness score, even though it seems a bit weaker than with the variables discussed so far.
```{r}
cor(whr$freedom_to_make_life_choices, whr$happiness_score)
```
In general, there is a strong positive correlation between the variables but it is the weakest so far.

### Generosity
```{r}
whr |>
  ggplot(aes(x = generosity, y = happiness_score)) +
  geom_point() +
  xlab("Generosity") +
  ylab("Happiness Score") +
  ggtitle("Happiness Score over Generosity") +
  theme(text = element_text(size = 15))
```
There is no connection between the variables visible here.

### Perceptions of corruption
```{r}
# fix data type of perceptions of corruption
whr$perceptions_of_corruption <- as.numeric(whr$perceptions_of_corruption)
whr |>
  ggplot(aes(x = perceptions_of_corruption, y = happiness_score)) +
  geom_point() +
  xlab("Perceptions of corruption") +
  ylab("Happiness Score") +
  ggtitle("Happiness Score over Perceptions of corruption") +
  theme(text = element_text(size = 15))
```
For perceptions of corruption, there seems to be a slight upward trend in happiness score for higher perceptions of corruption but it's only a fraction of data points that score high on perceptions of corruption. Also there is missing data for this variable.
```{r}
cor(whr$perceptions_of_corruption, whr$happiness_score, use = "complete.obs")
```
Calculationg the correlation shows that there is again a positive correlation between the two variables, even if it appears weak compared to other variables that have been looked at-

### Year
```{r}
whr$year <- as.factor(whr$year)
whr |>
  ggplot(aes(x = year, y = happiness_score, fill = year)) +
  geom_boxplot() +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        text = element_text(size = 15))+
  ggtitle("Happiness Score over Years") +
  ylab("Happiness Score")
```
For medians, there is a slight upwards trend over the years, which 2020 to 2022 having little variance compared to th other years. There are also negative outliers for 2021 to 2023.

## Using K-means Clustering to find interesting patterns
It is the goal to use K-means clustering to provide a pattern that is interpretable and give further context to the relationship between the happiness score and other variables. The procedure will first be introduced, then the data is scaled for an optimal performance and the best amount of clusters will be chosen by using the elbow-criteria to sensibly minimize total within sum-of-squares. At the end, a final model is trained and the result is plotted. <br>
It is important to mention that K-Means can only be used for continuous variables, so country, region, and year are not considered here.

### Introduction K-Means
K-Means Clustering is an unsupervised machine learning method that can help classify data that has no label that a model could be trained on. This is made possible by a distance-based approach that fits the clusters so that the distances between the data points within a cluster are minimal. The distance in this case is a measure of similarity between data points, so in other words we want to fit clusters so that the points contained in one cluster are as similar as possible.

### Prepare data for K-Means
First, we create a subset of the full data frame that only contains the numerical values. We also delete rows that contain NA-values.
```{r}
# create a subset that doesn't contain country, region and year
whr_sub <- whr[,-c(1,2,10)]

# delete rows that contain NAs from the data frame
whr_sub <- na.omit(whr_sub)
```
A good measure for improving the performance of K-Means is doing PCA on the data as it can aggregate the information. However, PCA is only useful if you can cover around 80% of the data with only few principle components.
```{r}
pr.out <- prcomp(whr_sub, scale = TRUE)

# get percent of variance of each principal component
perc <- pr.out$sdev/sum(pr.out$sdev)

# plot cumulative sum of the percentages
plot(cumsum(perc),
  xlab = "Principal Component",
  ylab = "Proportion of Variance Explained",
  main = "Cum. Prop. of Variance explained by each Principle Component",
  ylim = c(0,1),
  type = "b")
```
As can be seen, it would be necessary to use four to five of seven variables to cover a sufficient amount of variance. Since this isn't too helpful and each principle component contains a similar amount of variance, there will be no PCA used before doing K-Means.

### Finding a good amount of clusters
In general, the higher the number of clusters, the more similar points will be within one cluster. However, the model also gets harder to interpret and messy. So using the elbow-criteria, the hyperparameter k that equals the number of clusters will be sensibly minimized so that we get an interpretable result. <br>
We test the total within sum of squares for two to ten clusters.
```{r}
# scale data
sc_whr <- scale(whr_sub)

# initiate vector to store total within sum of squares for each kmeans()-object
wss <- c()

# run kmeans for k equals two to seven and save total within sum of squares
for (k in 2:7) {
  km.out <- kmeans(sc_whr, centers = k, nstart = 20)
  wss <- append(wss, km.out$tot.withinss)
}

# plot the resulting values
plot(wss, xlab = "k", ylab = "WSS", type = "b")
```
Using the elbow criterium, it can be seen that after k = 2, the decrease of within sum of squares isn't as strong anymore. Therefore, the final K-Means model is trained with two clusters.

### Training K-Means and plotting clusters
```{r}
# train K-Means
km.out <- kmeans(sc_whr, centers = 2, nstart = 20)

# plot resulting clustering
pairs(sc_whr, col = km.out$cluster, upper.panel = NULL, gap = 0)
```

